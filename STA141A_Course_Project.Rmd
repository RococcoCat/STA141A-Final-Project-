---
title: "STA141A Course Project: Mind Reading Mice"
author: "Claire Hsieh"
date: "2023-04-28"
output: 
  html_document:
    toc: true
    theme: united   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```

# STA 141A Course Project: Mind Reading Mice

# Abstract

Data science involves the use of statistical and computational tools to extract meaningful information from often messy and convoluted sets of data. It can also be used to make additional predictions on new data based on models built from our given dataset. For this project, we are given data from experiments in the paper: *Distributed coding of choice, action and engagement across the mouse brain* and are tasked with predicting trial feedback given the neural activity of a mouse. To do this, we must carry out a basic data science pipeline: exploratory analysis, data integration, and predictive modeling. We will then be given an unlabeled dataset for which we must correctly predict feedback. 

# Section 1 Introduction. 

- Introduce the objective and briefly review the background of this data set. 

The paper *Distributed coding of choice, action and engagement across the mouse brain* looked at the neuronal activity in four different mice as they made decisions based on visual contrast. The data includes each brain area's spike activity over the trial period, the contrast of the stimuli, and the feedback which could be 1 or -1 corresponding to a right or wrong answer. The goal of this project is to correctly predict the feedback given data on neuronal activity, time, and contrast using various classification and prediction methods. 


# Section 2 Exploratory analysis. 

```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(tibble))
suppressPackageStartupMessages(library(UpSetR))
suppressPackageStartupMessages(library(plyr))
suppressPackageStartupMessages(library("corrr"))
suppressPackageStartupMessages(library(ggcorrplot))
suppressPackageStartupMessages(library(ggfortify))
suppressPackageStartupMessages(library("factoextra"))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(PRROC))
suppressPackageStartupMessages(library(MLeval))
```

```{r, cache=TRUE}
# load dataset
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./Data/session',i,'.rds',sep=''))
  #print(session[[i]]$mouse_name)
  # print(session[[i]]$date_exp)
}

```




1. describe the data structures across sessions (e.g., number of neurons, number of trials, stimuli conditions, feedback types), 

```{r}
n.session=length(session)

# in library tidyverse
meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  n_brain_area = rep(0,n.session),
  n_neurons = rep(0,n.session),
  n_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)
colnames(meta)[3] = "brain areas"
colnames(meta)[4] = "neurons"
colnames(meta)[5] = "trials"

for(i in 1:n.session){
  tmp = session[[i]]
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
  
}
kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2) 
# print(meta)

# meta %>% ggplot(aes(x=neurons, y = success_rate, color=mouse_name)) +
#   geom_col(position="dodge")


# Scale data, place on the same graph?
# meta %>% ggplot() + 
  # geom_smooth(aes(x=`brain areas`, y = success_rate, color="red")) +
  # geom_smooth(aes(x=neurons, y = success_rate), color="blue") #+ 
  # geom_smooth(aes(x=trials, y = success_rate), color="green")
```

- From the summary table, we can see that four mice were tested, each with varying number of trials and success rates. Of the four mice, Lederberg seems to have the higheset success rate. 

- The stimuli conditions that the mice experiences are contrast on the left and right which can vary from 0 to 1. Feedback can be 1 for success and -1 for failure. 


### Descriptive Statistics

```{r}
# Get average summary statistics by mouse
summary_stat = data.frame(matrix(nrow=8, ncol=4))
rownames(summary_stat) = c("average success", "sd success","average brain areas", "sd brain areas", "average neurons",  "sd neurons", "average trials", "sd trials")

for (i in unique(meta$mouse_name)){
  summary_stat[1, i] = meta %>% filter(mouse_name==i) %>% select(success_rate) %>% apply(2, mean)
  summary_stat[2, i] = meta %>% filter(mouse_name==i) %>% select(success_rate) %>% apply(2, sd)
  summary_stat[3, i] = meta %>% filter(mouse_name==i) %>% select("brain areas") %>% apply(2, mean)
   summary_stat[4, i] = meta %>% filter(mouse_name==i) %>% select("brain areas") %>% apply(2, sd)
  summary_stat[5, i] = meta %>% filter(mouse_name==i) %>% select(neurons) %>% apply(2, mean)
  summary_stat[6, i] = meta %>% filter(mouse_name==i) %>% select(neurons) %>% apply(2, sd)
  
  summary_stat[7, i] = meta %>% filter(mouse_name==i) %>% select(trials) %>% apply(2, mean)
  summary_stat[8, i] = meta %>% filter(mouse_name==i) %>% select(trials) %>% apply(2, sd)

}

summary_stat = subset(summary_stat, select=-c(1:4))

kable(summary_stat, format = "html", table.attr = "class='table table-striped'",digits=2) 



# Plot success rate distribution
meta  %>% ggplot() + geom_boxplot(aes(mouse_name, success_rate)) + ggtitle("Success Rate")

# Plot brain areas distribution
# meta  %>% ggplot() + geom_boxplot(aes(mouse_name, `brain areas`)) + ggtitle("Brain Areas")
# not going to plot because I'm not sure all of the brain areas in meta are unique


# Plot neurons distribution
meta  %>% ggplot() + geom_boxplot(aes(mouse_name, neurons)) + ggtitle("Neurons")

# Plot trials distribution
meta  %>% ggplot() + geom_boxplot(aes(mouse_name, trials)) + ggtitle("Trials")

```

From these plots, we can see that Lederberg has a significantly higher success rate compared to the other mice. This could be important to note when we are creating predictive models as the neural activity of Lederberg may be more informative for accurately predicting feedback. We can see that Forssmann has the highest total number of neurons but this does not seem to affect its success rate, which implies that it is the identity of the brain area in which we see a spike that is correlated to the feedback. 



2. explore the neural activities during each trial

```{r}

i.s=17 # indicator for this session
i.t=1 # indicator for this trial

spk.trial = session[[i.s]]$spks[[i.t]]
area=session[[i.s]]$brain_area
# We need to first calculate the number of spikes for each neuron during this trial 
spk.count=apply(spk.trial,1,sum)

# Next we take the average of spikes across neurons that live in the same area 

# You can use tapply() or group_by() in dplyr

# tapply():
spk.average.tapply=tapply(spk.count, area, mean)

# dplyr: 
# To use dplyr you need to create a data frame
tmp <- data.frame(
  area = area,
  spikes = spk.count
)
# Calculate the average by group using dplyr
spk.average.dplyr =tmp %>%
  group_by(area) %>%
  summarize(mean= mean(spikes))

# Wrapping up the function:


average_spike_area<-function(i.t,this_session){

spk.trial = this_session$spks[[i.t]]
area= this_session$brain_area
spk.count=apply(spk.trial,1,sum)
spk.average.tapply=tapply(spk.count, area, mean)
return(spk.average.tapply)
}

# Test the function
average_spike_area(1,this_session = session[[i.s]])

n.trial=length(session[[i.s]]$feedback_type)
n.area=length(unique(session[[i.s]]$brain_area ))
# Alternatively, you can extract these information in the meta that we created before.

# We will create a data frame that contain the average spike counts for each area, feedback type,  the two contrasts, and the trial id

trial.summary =matrix(nrow=n.trial,ncol= n.area+1+2+1)


for(i.t in 1:n.trial){
  trial.summary[i.t,]=c(average_spike_area(i.t,this_session = session[[i.s]]),session[[i.s]]$feedback_type[i.t],session[[i.s]]$contrast_left[i.t],session[[i.s]]$contrast_left[i.s],i.t)

}
colnames(trial.summary)=c(names(average_spike_area(i.t,this_session = session[[i.s]])), 'feedback', 'left contr.','right contr.','id' )
# id corresponds to trial # 

# Turning it into a data frame
trial.summary <- as_tibble(trial.summary)

area.col=rainbow(n=n.area,alpha=0.7) # color code entries in n.area
# In base R, I usually initiate a blank plot before drawing anything on it
plot(0~1, col='white',xlim=c(0,n.trial),ylim=c(0.5,4), xlab="Trials",ylab="Average spike counts", main=paste("Spikes per area in Session", i.s))
for(i in 1:n.area){
  lines(y=trial.summary[[i]],x=trial.summary$id,col=area.col[i],lty=2,lwd=1)
  lines(smooth.spline(trial.summary$id, trial.summary[[i]]),col=area.col[i],lwd=3)
  }

legend("topright", 
  legend = colnames(trial.summary)[1:n.area], 
  col = area.col, 
  lty = 1, 
  cex = 0.8
)

```

- This dotted lines of this graph shows the average spike count for each unique brain area of the Lederberg mice in session 17 and the solid lines are the smoothed average of the average spike counts. We can see that there is some correlation between the areas since MEA and root as well as LD and VPL seem to follow the same patterns. We can also see that there is somewhat of a trend towards the beginning and end of the trials where LD seems to increase. 



3.explore the changes across trials

```{r}
plot.trial<-function(i.t,area, area.col,this_session){
    spks=this_session$spks[[i.t]];
    n.neuron=dim(spks)[1]
    time.points=this_session$time[[i.t]]
    
    plot(0,0,xlim=c(min(time.points),max(time.points)),ylim=c(0,n.neuron+1),col='white', xlab='Time (s)',yaxt='n', ylab='Neuron', main=paste('Trial ',i.t, 'feedback', this_session$feedback_type[i.t] ),cex.lab=1.5)
    for(i in 1:n.neuron){
        i.a=which(area== this_session$brain_area[i]);
        col.this=area.col[i.a]
        
        ids.spike=which(spks[i,]>0) # find out when there are spikes 
        if( length(ids.spike)>0 ){
            points(x=time.points[ids.spike],y=rep(i, length(ids.spike) ),pch='.',cex=2, col=col.this)
        }
    }
    
legend("topright", 
  legend = area, 
  col = area.col, 
  pch = 16, 
  cex = 0.8
  )
  }
varname=names(trial.summary);
area=varname[1:(length(varname)-3)]
plot.trial(1,area, area.col,session[[i.s]])
```

```{r}
varname=names(trial.summary);
area=varname[1:(length(varname)-3)]
par(mfrow=c(1,2))
plot.trial(1,area, area.col,session[[i.s]])
plot.trial(2,area, area.col,session[[i.s]])
```
```{r}
par(mfrow=c(1,1))

```


Every dot corresponds to a value on the spike vs time graph. 

- We can see that the root neuron expression is much more common compared to the other neurons. We can also see that expression of other neurons occur less often but when they do occur, they seem to be constant across that trial. 


4. explore homogeneity and heterogeneity across sessions and mice. 

```{r}

# Get brain areas present in each session for each mouse
brains = c()
for (i in 1:18){
  tmp = session[[i]]
  brains = unique(append(brains, unique(tmp$brain_area)))
}

brain.areas.s = data.frame(matrix(data=0, ncol = 4, nrow = length(brains)))
colnames(brain.areas.s) = c(unique(meta$mouse_name))
rownames(brain.areas.s) = brains
for(i in 1:18){
  tmp = session[[i]]
  m = tmp$mouse_name
  for(j in tmp$brain_area){
    brain.areas.s[j, m] = 1
  }
}
areas = c("Visual cortex", "Frontal cortex", "Hippocampus", "Basal ganglia", "Thalamus", "Midbrain")
# head(brain.areas.s)

upset(brain.areas.s)

```

This graph shows the brain areas common among the different mice. We can see that Lederberg has the largest number of active brain sites and has a column with many neurons that are unique only to it (n=17). Cori has the smallest number of brain areas (as can be seen by the smaller set size on the left) and doesn't have many unique brain areas. Forssmann and Hench both have many of the common brain areas as well as some other intersections as can be seen by many of the middle columns. This matches what we see in the average success rate of the four mice since Cori seems to have the lowest success rate and Lederberg the highest. 


# Section 3 Data integration. 

## Organizing brain area data by mouse

We are interested in predicting feedback based on mouse, the contrast of the trial, and the spike firing data of that trial. Based on the Time vs Neuron graph above, there does not appear to be much of a pattern in the timing of the spike firing. Data analysis would be much easier if we were to reduce the dimensions of the spike firing data from 2D (time vs spike) to 1D (spike) so I have decided to remove the time data. In addition, the paper frequently uses the spike firing *rate* when performing data analysis which similarly removes the time component from their data. 

I will be creating a dataframe named `mouse.spk.freq` that contains the spike frequency for each brain area during each trial for all sessions. Even though we observe the activation of different brain areas throughout the different sessions, I believe it is acceptable to combine all the data into one dataframe since I will be using the *sum* of the spike frequencies in each trial. This means that 0's correspond to not seeing the brain area at all and there would essentially be no difference between NA's and 0's. 

```{r, cache=TRUE}
# function to get frequency of spikes by brain area across trials
# input = session number, dataframe to be outputted
# output = dataframe with columns corresponding to brain areas, rows corresponding to trial
# cells = frequency of spike in brain area
spk_freq <- function(ses, mouse_df, session_data){
  n.session1.brain = length(unique(session_data[[ses]]$brain_area))
  n.session1 = length(session_data[[ses]]$feedback_type)
  mouse_df = data.frame(matrix(ncol=n.session1.brain))
  colnames(mouse_df) = unique(session_data[[ses]]$brain_area)
  mouse_df = mouse_df[-1,]
  # loop through trials
  # loop through columns (time) in spike datasets and count freq of neurons
  this_session = session_data[[ses]]
  n.time = length(session_data[[ses]]$time[[1]])
    
  for(i.t in 1:n.session1){
    spks=this_session$spks[[i.t]];
    n.neuron=dim(spks)[1]
    time.points=this_session$time[[i.t]]
    trial_sum = data.frame(matrix(ncol=n.session1.brain))
    colnames(trial_sum) = unique(session_data[[ses]]$brain_area)
    for(i in 1:n.time){
        i.a=which(area== this_session$brain_area[i]);
        col.this=area.col[i.a]
        ids.spike=which(spks[,i]>0) # select rows with spikes
        row_count = table(session_data[[ses]]$brain_area[ids.spike]) # get frequency of spike for each brain area
        if(length(row_count) > 0){
          trial_sum = rbind.fill(trial_sum, data.frame(rbind(row_count)))
        }
    }
    trial_sum = data.frame(t(colSums(trial_sum, na.rm = TRUE))) # sum freq of spikes in each trial
    mouse_df = rbind.fill(mouse_df, trial_sum)
  }
  mouse_df = cbind(session_data[[ses]]$contrast_left,mouse_df)
  mouse_df = cbind(session_data[[ses]]$contrast_right,mouse_df)
  mouse_df = cbind(session_data[[ses]]$feedback_type,mouse_df)
 

  names(mouse_df)[names(mouse_df) == "session[[ses]]$feedback_type"] <- "feedback"
  names(mouse_df)[names(mouse_df) == "session[[ses]]$contrast_left"] <- "contrast_left"
  names(mouse_df)[names(mouse_df) == "session[[ses]]$contrast_right"] <- "contrast_right"

  # names(mouse_df)[names(mouse_df) == "ses_n"] <- "session"
  return(mouse_df)
}


# Create dataframe of spike freq for each mouse
mouse.df <- function(ses_ls, mouse_df, session_data){
  tmp1 = spk_freq(ses_ls[1], tmp, session_data)
  for (i in ses_ls[2:length(ses_ls)]){
    tmp = spk_freq(i, tmp, session_data)
    if(i == ses_ls[2]){
      mouse_df = rbind.fill(tmp1, tmp)
    }
    else{
      mouse_df = rbind.fill(mouse_df, tmp)
    }
  }
  return(mouse_df)
}

# Add the session number to our dataset
ses_ls = c(1:3)
cori.df = data.frame()
cori.df = mouse.df(ses_ls, cori.df, session)
cori.df[is.na(cori.df)] <- 0

ses_ls = c(4:7)
forssmann.df = data.frame()
forssmann.df = mouse.df(ses_ls, forssmann.df, session)
forssmann.df[is.na(forssmann.df)] <- 0

ses_ls = c(8:11)
hench.df = data.frame()
hench.df = mouse.df(ses_ls, hench.df, session)
hench.df[is.na(hench.df)] <- 0

ses_ls = c(12:18)
lederberg.df = data.frame()
lederberg.df = mouse.df(ses_ls, lederberg.df, session)
lederberg.df[is.na(lederberg.df)] <- 0

# Use functions above to get spike data and mouse name
mouse.spk.freq = data.frame()
mouse_nm = rep("Cori", dim(cori.df)[1])
mouse.spk.freq = rbind.fill(mouse.spk.freq, cbind(mouse_nm, cori.df))
mouse_nm = rep("Forssmann", dim(forssmann.df)[1])
mouse.spk.freq = rbind.fill(mouse.spk.freq, cbind(mouse_nm, forssmann.df))
mouse_nm = rep("Hench", dim(hench.df)[1])
mouse.spk.freq = rbind.fill(mouse.spk.freq, cbind(mouse_nm, hench.df))
mouse_nm = rep("Lederberg", dim(lederberg.df)[1])
mouse.spk.freq = rbind.fill(mouse.spk.freq, cbind(mouse_nm, lederberg.df))
mouse.spk.freq[is.na(mouse.spk.freq)] = 0


colnames(mouse.spk.freq)[2] = "feedback"
colnames(mouse.spk.freq)[3] = "contrast_right"
colnames(mouse.spk.freq)[4] = "contrast_left"

# Benchmark model
ses_ls = c(1, 18)
benchmark = data.frame()
benchmark = mouse.df(ses_ls, benchmark, session)
benchmark[is.na(benchmark)] <- 0

colnames(benchmark)[1] = "feedback"
colnames(benchmark)[2] = "contrast_right"
colnames(benchmark)[3] = "contrast_left"
```

```{r}
kable(head(mouse.spk.freq), format = "html", table.attr = "class='table table-striped'",digits=2) 

```

### Visualizing patterns in our data


```{r, fig.width=10, fig.height=20}

# Plot all brain areas across all mice
mouse.spk.freq[,-c(2:4)] %>% gather("area", "count", 2:63) %>% ggplot() +
  geom_col(mapping=aes(x=area, y=count, fill=as.factor(area))) +
  facet_wrap(vars(mouse_nm)) +
  scale_y_continuous(guide=guide_axis(n.dodge = 1)) +
  coord_flip() +
  guides(fill=guide_legend(title="Brain Area"))


``` 



```{r}
# Plot common brain areas

brain.areas.s$common_brain_areas = rowSums(brain.areas.s)==4
common_brain_areas = brain.areas.s %>% filter(common_brain_areas==TRUE) %>% row.names()

mouse.spk.freq[,-c(2:4)] %>% gather("area", "count", 2:63) %>% filter(area %in% common_brain_areas) %>% ggplot() +
  geom_col(mapping=aes(x=area, y=count, fill=as.factor(area))) +
  facet_wrap(vars(mouse_nm)) +
  guides(fill=guide_legend(title="Brain Area"))

```

The first graph shows the frequency of all the different brain areas observed throughout the entire experiment and the second graph shows the frequency of only the brain areas that are common to all the mice. We can see that the root neuron is very highly expressed in Lederberg and Forssman, both of which have higher success rates than the other mice. One theory for the higher success rates could be higher neural activity in some of the shared brain areas or unique neural activity in a mice that shows greater success in the tested activity (Lederberg).

```{r, fig.width=10, fig.height=20}
# Plot brain areas by feedback
mouse.spk.freq[,-c(1,3,4)] %>% gather("area", "count", 2:dim(mouse.spk.freq[,-c(1,3,4)])[2]) %>% ggplot() +
  geom_col(mapping=aes(x=area, y=count, fill=as.factor(area))) +
  facet_wrap(vars(feedback)) +
  # scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(guide=guide_axis(n.dodge = 1)) +
  coord_flip() +
  guides(fill=guide_legend(title="Brain Area"))

```

Based on this graph of the distribution of brain area activation based on feedback, we can see that in trails with positive feedback, there is overall greater brain area activation compared to trials with negative feedback even if the distribution is similar. From this we can infer that it is likely not the specific brain areas that are correlated with feedback but rather, the number of spikes in each trail, regardless of the brain area where they're found.



```{r, fig.width=10, fig.height=20}
# # Plot brain areas by feedback
# mouse.spk.freq[,-c(1,3,4)] %>% gather("area", "count", 2:dim(mouse.spk.freq[,-c(1,3,4)])[2]) %>% ggplot() +
#   geom_col(mapping=aes(x=area, y=count, fill=as.factor(area))) +
#   facet_wrap(vars(feedback)) +
#   # scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
#   scale_y_continuous(guide=guide_axis(n.dodge = 1)) +
#   coord_flip() +
#   guides(fill=guide_legend(title="Brain Area"))

# This is a plot of brain areas by feedback with the outliers removed. As we can see, there are much less brain areas compared to the previous graph.


```

### Data Cleaning

```{r}
# Check for linearly dependent columns
feature_map <- unlist(lapply(mouse.spk.freq, is.numeric)) 
findLinearCombos((mouse.spk.freq[,feature_map]))
```
Since the goal of this project is to predict feedback using brain areas, linearly dependent columns would not be useful in our models as they would not provide any new information and should therefore be removed to reduce the complexity of our model. We do not see any linearly dependent columns in our dataset. 


```{r}
# Check for imbalance in classes
table(mouse.spk.freq$feedback)

```

We can see that there is a greater number of trials with positive compared to negative feedback. This is important to note as having imbalanced classes could compromise our models if we do not address this problem either when building the model or when sampling. 

```{r, results="hide", fig.keep = "none"}
# # Remove outliers
# dim(mouse.spk.freq)
# original.data = mouse.spk.freq
# 
# # create detect outlier function
# detect_outlier = function(x) {
#     Quantile1 = quantile(x, probs=.25)
#     Quantile3 = quantile(x, probs=.75)
#     IQR = Quantile3-Quantile1
#     x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
# }
# 
# # create remove outlier function
# remove_outlier = function(dataframe,                        columns=names(dataframe)) {
# 
#     for (col in columns) {
#         dataframe = dataframe[!detect_outlier(dataframe[[col]]), ]
#     }
#     print(dataframe)
# }
# mouse.spk.freq = remove_outlier(mouse.spk.freq, colnames(mouse.spk.freq)[-1])
# mouse.spk.freq[is.na(mouse.spk.freq)] <- 0
# 
# dim(mouse.spk.freq)
# 
# # Remove columns with all 0's
# mouse.spk.freq = mouse.spk.freq[colSums(mouse.spk.freq != 0) > 0]
# 
# dim(mouse.spk.freq)
```
 



## Dimension Reduction

The purpose of Dimension Reduction is to find any useful patterns in the data or to simply our models by filtering out useful predictor variables. I will be PCA to find informative predictor variables. 

### PCA

I will perform PCA on each of the different mice to see if they share the same brain areas that could be used in a prediction model. 

```{r}
# PCA

mouse.spk.freq.pca <- mouse.spk.freq[,6:length(mouse.spk.freq)]  %>% prcomp(center = TRUE, scale = TRUE)
autoplot(mouse.spk.freq.pca, data = mouse.spk.freq.pca, loadings = TRUE, loadings.colour = 'blue',loadings.label = TRUE, loadings.label.repel=TRUE)

# fviz_pca_var(mouse.spk.freq.pca, col.var = "black", repel=TRUE)
fviz_eig(mouse.spk.freq.pca, addlabels = TRUE)
fviz_pca_var(mouse.spk.freq.pca, col.var = "cos2",
            gradient.cols = c("black", "gold", "mediumpurple"),
            repel = TRUE)

```

When we perform PCA, one brain area stands out on the cos2 plot. The cos2 plot shows the importance of each vector in creating the PCs. Principle components seek to explain variation in data by combining predictor variables. 

The PCs generated by PCA do not appear useful as each PC seems to explain an equal amount of variance in our data. Based on the scree plot, it is difficult to determine the number of PCs we should include in our model. Therefore I will not be using the PCs generated by PCA for modeling as they do not appear to be an improvement upon using the given brain areas as predictors. In addition, PCs are more difficult to interpret as they are a combination of the brain areas.   


# Section 4 Predictive modeling. 

Finally, I will build a prediction model to predict the outcome (i.e., feedback types). Since we have access to labeled data, it would be best to choose supervised machine learning models as they generally outperform unsupervised models in terms of accuracy. I will test a few different types of models including: 

1. **Gradient Descent Based Algorithms**: Gradient descent based algorithms try to minimize the standard error of the model in steps.  


- Logistic regression: I chose to use logistic regression because this model is ideal for classifying binary data. Logistic regression also explains its choice in classification because it uses a maximum likelihood estimation to generate the probability of trials being in certain classes, then classifying based on those probabilities. 


2. **Dimensionality Reduction**: Dimensionality reduction algorithms seek to reduce the number of features of a model, while keeping features that retain the most information in the dataset.

- Linear Discriminant Analysis (LDA): Since we have so many potential predictor variables, LDA could be useful in removing several, uninformative predictors. 



3. **Distance based algorithms**: Distance based algorithms attempt to classify data based on their relative distances. 


- k-nearest neighbors (knn): knn is an unsupervised learning model, works well on non-linear data, and may allow us to determine the importance of different variables on predicting feedback.

- Support Vector Machine (SVM): SVM is a supervised model and performs well in high dimensional spaces (when there is a large number of parameters).


4. **Tree based algorithms**: Tree based algorithms classify data using decision trees. 

- Random forest: Random forest tends to be highly accurate and handles outliers and noise well, though it may be computationally intensive. 


The performance of my models will be compared to a benchmark model of 330 trials based on session 1 and session 18. 



## Create Training and Testing Dataset Using Split-Sample Validation

For one of the models I plan on using (LDA), there is an assumption that the data has a normal distribution and for that reason I will first normalize my data. 

```{r}

set.seed(1)

# Normalize data
process <- preProcess(as.data.frame(mouse.spk.freq), method=c("range"))
mouse.norm =  predict(process, as.data.frame(mouse.spk.freq))
mouse.norm = mouse.norm[,-1]

# Training = 80% of data, Testing = 20% of data
##Generate a random number that is 80% of the total number of rows in dataset.
ran <- sample(1:nrow(mouse.spk.freq), 0.8 * nrow(mouse.spk.freq)) 
mouse.train = mouse.norm[ran,]
mouse.test = mouse.norm[-ran,]

# Benchmark
process <- preProcess(as.data.frame(benchmark), method=c("range"))
benchmark.norm =  predict(process, as.data.frame(benchmark))

ran <- sample(1:nrow(benchmark), 0.8 * nrow(benchmark)) 
benchmark.train = benchmark.norm[ran,]
benchmark.test = benchmark.norm[-ran,]


# Upsample dataset
up <- upSample(x=mouse.train[,-1], y=factor(mouse.train$feedback), yname="feedback")
# count(up$feedback)
mouse.train = up %>% relocate(feedback)

# [5],[26]
```

Because our dataset has more positive than negative feedback, our dataset is imbalanced and will therefore perform better when predicting positive feedback. To address this issue, I have upsampled the training dataset so that there is an equal number of both positive and negative feedback.

## Building a Model

Since unsupervised learning is not very useful for classifying or making predictions, it would be more useful to use supervised learning which takes labeled data (feedback) and uses it to train a model to make predictions based on brain areas.

### Benchmark

For the benchmark model, I created a logistic regression model using the data extracted from session 1 and 18 to create a benchmark training set then tested this model on the same test set I use for the other models. 

```{r}
# Benchmark
set.seed(1)

# Create benchmark model with benchmark.norm, test with test set used by other models (mouse.test)
log_model.benchmark <- glm(feedback ~ .,data = benchmark.norm, family = "binomial")
predictions_log.bench <- predict(log_model.benchmark, mouse.test, type="response")
predicted_feedback_log_bench <- (predictions_log.bench>0.5)*1
# summary(log_model.benchmark)

cm.log.bench = confusionMatrix(factor(mouse.test$feedback),factor(predicted_feedback_log_bench), mode = "everything")
print(cm.log.bench)

# ROC
pred <- prediction(predictions_log.bench, mouse.test$feedback)
perf <- performance(pred, "tpr", "fpr")
plot(perf)
abline(0, 1, col="red")
auc(mouse.test$feedback, predictions_log.bench, main = "ROC Curve")

```

### Logistic Regression

```{r}
# Logistic Regression

set.seed(1)
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

# Build log model
log.model <- glm(mouse.train$feedback ~ .,data = mouse.train, family = "binomial")

# Get predictions
predictions.log <- predict(log.model, mouse.test, type="response")

# Choose cut off point
pcut1<- mean(mouse.train$feedback)

# get binary prediction and confusion matrix
predicted.feedback.log <- (predictions.log>0.5) * 1
# summary(log.model)

cm.log = confusionMatrix(factor(mouse.test$feedback),factor(predicted.feedback.log), mode = "everything")
print(cm.log)


```

I chose to use a logistic regression model because it is well suited for categorical data and since the data I am trying to classify is binary, this will be an especially simple model to create. Logistic regression is a classification algorithm and will give us the probabilities of samples belonging to certain classes or having certain algorithms. Therefore it is up to us to choose a probability cutoff for a sample belonging to a certain class. This can be done with a variety of algorithms or the cutoff can simply be set to 50% since our predicted variable is binary. Ideally though, we should choose a cutoff that maximizes the sensitivity-specificity metric.

The miscalculation rate of our logistic regression model on a normalized training and testing dataset depends on the training and testing dataset so after performing logistic regression on 100 different training and testing datasets, our average accuracy is 59.72%. Given that our data only has two possible outcomes (0 and 1), the expected accuracy is 50%, which means our model is somewhat better than random chance. 

#### Sensitivity Analysis

```{r}
# ROC and AUC: Logistic Regression
pred <- prediction(predictions.log, mouse.test$feedback)
perf <- performance(pred, "tpr", "fpr")
plot(perf,col="red", main = "ROC Curve for Logistic Regression")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
auc(mouse.test$feedback, predictions.log, main = "ROC Curve")


# PR Curve
pred.glm0.train<- predict(log.model, type="response")
score1= pred.glm0.train[mouse.train$feedback==1]
score0= pred.glm0.train[mouse.train$feedback==0]
pr= pr.curve(score1, score0, curve = T)
pr
plot(pr, main="In-sample PR curve")

# Out-of-sample prediction: 
pred_glm0_test<- predict(log.model, newdata = mouse.test, type="response")
score1.test= pred_glm0_test[mouse.test$feedback==1]
score0.test= pred_glm0_test[mouse.test$feedback==0]
pr.test= pr.curve(score1.test, score0.test, curve = T)
pr.test
plot(pr.test, main="Out-of-sample PR curve")




```


The ROC (receiver operating characteristic curve) plots the true positive rate against the false positive rate. This could also be thought of as a sensitivity (true positive) vs specificity (false positive) graph. Our ROC curve is somewhat close to the y=x line, which would represent a random model. The AUC of our model is 0.6615. A perfectly accurate model would have an AUC of 1 and curve all the way to 1 on the y axis, which means that our model is not a very accurate predictor of feedback.

The PR (precision-recall) curve is better suited to inbalanced classes which is what we observe in our dataset. It shows the relationship between precision (positive predictive power) and recall (sensitivity). Based on the PR curve, we can see that our model has similar performance on both the training and testing dataset. The AUC of the in-sample (training) set is 0.8277268 and the AUC of the out-sample (test) set is 0.8109447. This would mean that overfitting is not a significant issue in our model since the model performs similarly on both training and testing datasets. 

#### Interaction term

Because a mouse is choosing between two contrasts on the left and right, there will be an interaction between the contrast_left and contrast_right variable, especially if both are non-zero at the same time. If a mouse has to choose between two non-zero contrasts, the difficulty of the trial will be greater, and the chance of negative feedback will also be greater. Therefore we should also include an interaction term between left and right contrast. It would also make sense to include interaction terms between brain areas that are in the same brain region (ie visual cortex) but because there are so many brain areas, including interaction terms for all of them, might make the model unnecessarily complex. 

```{r}
set.seed(1)
# Log Regression with Interaction

log_model.interaction <- glm(mouse.train$feedback ~ . + contrast_right * contrast_left, data = mouse.train, family = "binomial")
predictions_log1.interaction <- predict(log_model.interaction, mouse.test, type="response")
predicted_feedback_log1_interaction <- (predictions_log1.interaction>0.5)*1
cm.log.interaction = confusionMatrix(factor(mouse.test$feedback),factor(predicted_feedback_log1_interaction), mode = "everything")
print(cm.log.interaction)
```


### Linear Discreminant Analysis (LDA)

Linear Discriminant Analysis (LDA) is used to classify data by projecting data onto a lower dimensional space, then attempts to maximize the difference between classes. To use LDA, our data must have a Gaussian distribution and the covariance matrices of the different classes must be equal. LDA can 

```{r}
# LINEAR DISCREMINANT ANALYSIS [6]
library(MASS)
set.seed(1)

theme_set(theme_classic())

lda_model <- lda(feedback~., data = mouse.train)
lda_predictions <- lda_model %>% predict(mouse.test)
predicted_feedback_lda = lda_predictions$class
cm.lda  = confusionMatrix(as.factor(mouse.test$feedback),as.factor(predicted_feedback_lda), mode = "everything")
cm.lda
```


#### Sensitivity Analysis

```{r}

# ROC and AUC: LDA
pred <- prediction(lda_predictions$posterior[,2], mouse.test$feedback)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col="red",main = "ROC Curve for LDA")
abline(a=0,b=1,lwd=2,lty=2,col="gray")
auc(mouse.test$feedback, lda_predictions$posterior[,2])

# PR Curve
lda_model.train <- predict(lda_model, type="response")
score1= lda_model.train$x[mouse.train$feedback==1]
score0= lda_model.train$x[mouse.train$feedback==0]
pr= pr.curve(score1, score0, curve = T)
plot(pr, main="In-sample PR curve")

# Out-of-sample prediction: 
lda_model.test <- predict(lda_model, type="response")
score1.test= lda_model.test$x[mouse.test$feedback==1]
score0.test= lda_model.test$x[mouse.test$feedback==0]
pr.test= pr.curve(score1.test, score0.test, curve = T)
plot(pr.test, main="Out-of-sample PR curve")


```

LDA performs somewhat better than logistic regression with an average accuracy of 73.08% but still isn't very accurate. The AUC for our LDA model is 0.6564 which would not qualify as a "good model" as a good model should have an AUC above 0.7. A perfect model would have an AUC of 1 which means it is perfectly accurate but would also very likely be be overfit. 


### k-Nearest Neighbors (knn)

knn is a non-parametric algorithm meaning it will perform well if our data is non linear. Non-parametric algorithms make "no assumptions ... about the shape of the decision boundary". [30] knn works by computing the distance between each datapoint to all other points then selecting k number of nearest neighbors and classifies based on the most frequent label. 

We will first perform k-fold cross validation to determine the ideal k to be used for our knn model. k-fold Cross Validation is a technique used to evaluate the performance of a model by splitting the data into k subsets and training the model on k-1 subsets and testing it on the remaining subset. This is repeated k times and the average accuracy is calculated.


```{r, cache=TRUE}
# k-Nearest Neighbors

library(class)

repeats = 5
numbers = 10
tunel = 20

mouse.norm$feedback = factor(mouse.norm$feedback)
levels(mouse.norm$feedback) <- make.names(levels(factor(mouse.norm$feedback)))

# k-fold cross validation to choose optimal k
x = trainControl(method = 'repeatedcv',number = numbers,repeats = repeats,classProbs = TRUE,
summaryFunction = twoClassSummary)

knn.model <- train(feedback~. , data = mouse.norm, method = 'knn',
                preProcess = c('center','scale'),
                trControl = x,
                metric = 'ROC',
                tuneLength = tunel)
knn.model
plot(knn.model)

knn.pred <- predict(knn.model,mouse.test[,-1], type = 'prob')
knn.pred.feedback <-prediction(knn.pred[,2],mouse.test$feedback)

cm.knn  = confusionMatrix(as.factor(mouse.test$feedback),as.factor((knn.pred[,2]>0.5)*1), mode = "everything")
print(cm.knn)
```

Here, I use ROC (repeated cross validation) as the metric used to train the knn model. ROC is a bootstrapping method that allows the training dataset given to the model to be further split into training and testing datasets that will be used to improve the model. This spliting and training will then be done some specified number of times. Here I have decided to have the model repeat this process 5 times. Tune length species the number of tuning parameters the model should try to obtain the best performance. For a knn model, one of the most important parameters is k or the number of nearest neighbors and the `trainControl` will automatically select these parameters for us. 

Because knn "relies on observable data similarities and sophisticated distance metrics to generate accurate predictions" [29], it does not require a training dataset and so I simply used the normalized mouse dataset to generate the model. The normalization step might not be required as the training function will likely preprocess the dataset anyway. 

#### Sensitivity Analysis

```{r}
# ROC and AUC: knn
perf_val <- performance(knn.pred.feedback,'auc')
perf_val

perf_val <- performance(knn.pred.feedback, 'tpr', 'fpr')
plot(perf_val, col="red", lwd = 1.5, main = "ROC curve for knn")
abline(a=0,b=1,lwd=2,lty=2,col="gray")

knn.predicted = (knn.pred[,2]>0.5)*1

auc(mouse.test$feedback,knn.predicted)

```


### Random Forest

Random forest is a machine learning model similar to decision trees but with the addition of bootstrapping to generate multiple sample datasets. Bootstrapping will select a pre-determined number of datapoints with replacement which may leave out certain datapoints. The datapoints that are not included in the training data are used to determine the out of bag error rate (OOB) which is the error rate of the random forest model on these datapoints. To select mtry (number of features to consider at each split point), we use the OOB as a cost function, and attempt to select the mtry that minimizes the OOB. 

The random forest model does not have underlying assumptions about the data and therefore have increased flexibility and possibly accuracy compared to regression models (logistic regression) and overcomes the issue of over-fitting that is common with decision trees. Random forest models are also simpler to interpret compared to other non parametric models as it is essentially an amalgamation of decision trees which explain the decisions for classification based on each node. This allows us to generate a feature importance plot which tells us the importance of each feature for predictions. 

```{r, cache=TRUE}
# Random Forest

library(randomForest)
set.seed(1)

# Select mtry value with minimum out of bag(OOB) error.
mtry <- tuneRF(mouse.train[-1],mouse.train$feedback, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)

# build model with best mtry value
rf <-randomForest(as.factor(feedback)~.,data=mouse.train, mtry=best.m, importance=TRUE,ntree=500)
print(rf)

#Evaluate variable importance
importance(rf)
varImpPlot(rf)

# prediction
rf.pred <- predict(rf, mouse.test[,-1], type = 'prob')
rf.predicted = (rf.pred[,2]>0.5) * 1

# Confusion matrix
cm.rf = confusionMatrix(factor(mouse.test$feedback),factor(rf.predicted), mode = "everything")
print(cm.rf)


```


The variable importance plot shows the importance of each variable in classifying the data represented as the decrease in accuracy if the predictor variable were to be excluded from the model (MeanDecreaseAccuracy) and the variable's contribution to the homogeneity of the nodes and leaves in the model (MeanDecreaseGini). Higher vales in the mean decrease of Gini coefficient means that the variable produces more discriminative nodes and higher values in both graphs correlates with greater importance of that predictor variable. As we can see in the graphs, contrast, root and CA3 are very important in producing the random forest model. [28]


#### Sensitivity Analysis

```{r}
# prediction
perf = prediction(rf.pred[,2], mouse.test$feedback)

# 1. Area under curve
auc = performance(perf, measure = "auc")
auc
# 2. True Positive and Negative Rate
pred3 = performance(perf, "tpr","fpr")
# 3. Plot the ROC curve
plot(pred3,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

auc(mouse.test$feedback,rf.predicted)
```



### Support Vector Machine (SVM)

SVMs work by projecting data into a higher dimensional space, where it will then find a hyperplane that can accurately seperate the points while maximizing margins. Different kernels can be selected (kernel function) for how the data is mapped into the higher dimensional space. Here I have chosen radial basis function (RBF) because it is better at handling nonlinear relationships between class and predictor variables. [26] I also chose to use C-classification as the SVM type as our goal is to use this model to classify feedback. 

```{r, cache=TRUE}
# Support Vector Machine

library(caTools)
library(e1071)
set.seed(1)

svm.model = svm(formula = feedback ~ .,
                 data = mouse.train,
                 type = 'C-classification', 
                 kernel = 'radial')

# Predicting the Test set results
svm.pred = predict(svm.model, newdata = mouse.test[-1])

# Making the Confusion Matrix
cm.svm  = confusionMatrix(as.factor(mouse.test$feedback),as.factor(svm.pred), mode = "everything")
cm.svm

```

#### Sensitivity Analysis


```{r}
# plot(svm.model, mouse.norm.train, root ~ CA3)

# ROC
roc_svm_test <- roc(response = mouse.test$feedback, predictor =as.numeric(svm.pred))

plot(roc_svm_test, col="red", lwd = 1.5, main = "ROC Curve for SVM")
auc(mouse.test$feedback,as.numeric(svm.pred))


# [25]
```

# Section 5 Prediction performance on the test sets. 

## Testing the data

```{r}
# Read in test data
test_session=list()
for(i in 1:2){
  test_session[[i]]=readRDS(paste('./Data/test/test',i,'.rds',sep=''))
  # print(test[[i]]$mouse_name)
  # print(test[[i]]$date_exp)
}

# Format test as dataframe
ses_ls = c(1, 2)
test = data.frame(matrix(ncol=length(colnames(mouse.norm))))
test_tmp = data.frame()
colnames(test) = colnames(mouse.norm)
test_tmp = mouse.df(ses_ls, test_tmp, test_session)
colnames(test_tmp)[1] = "feedback"
colnames(test_tmp)[2] = "contrast_right"
colnames(test_tmp)[3] = "contrast_left"
test = right_join(test, test_tmp)
test[is.na(test)] <- 0

# head(test)

table(test$feedback)

# Normalize data
process <- preProcess(as.data.frame(test), method=c("range"))
test.norm =  predict(process, as.data.frame(test))

# Predict feedback using knn model
knn.pred.test <- predict(knn.model, test.norm[,-1], type = 'prob')
knn.pred.test.feedback <-prediction(knn.pred.test[,2], test.norm$feedback)
cm.knn.test  = confusionMatrix(as.factor(test.norm$feedback),as.factor((knn.pred.test[,2]>0.5)*1), mode = "everything")
knn.predicted.test = (knn.pred.test[,2]>0.5)*1

print(cm.knn.test)

```


## Model Accuracy

```{r}
# Model Accuracy

get_perf_metrics <- function(df.test, pred, cm, model_name){
  row = c(model_name, 
round(cm$overall["Accuracy"]*100, digits=2), 
round(cm$byClass["Balanced Accuracy"]*100, digits=2),
round(cm$byClass["F1"]*100, digits=2),
round(auc(df.test$feedback, pred), digits=3),
round(cm$byClass["Sensitivity"]*100, digits=2), 
round(cm$byClass["Specificity"]*100, digits=2), 
round(cm$byClass["Precision"]*100, digits=2), 
round(cm$byClass["Recall"]*100, digits=2))
}
acc = data.frame(matrix(ncol=9))
colnames(acc) = c("Model", "Accuracy (%)", "Balanced Accuracy(%)", "F1-Score", "AUC", "Sensitivity", "Specificity", "Precision", "Recall")

# Test set
acc[nrow(acc), ] = get_perf_metrics(test, knn.predicted.test, cm.knn.test, "Test Set (knn)")

acc[nrow(acc)+1, ] = get_perf_metrics(mouse.test, predictions_log.bench, cm.log.bench, "Benchmark (Logistic regression)")
acc[nrow(acc)+1,] = get_perf_metrics(mouse.test, predictions.log, cm.log, "Logistic Regression")
acc[nrow(acc)+1,] = get_perf_metrics(mouse.test, predictions_log1.interaction, cm.log.interaction, "Logistic Regression (with interaction)")
acc[nrow(acc)+1,] = get_perf_metrics(mouse.test, lda_predictions$posterior[,2], cm.lda, "LDA")
acc[nrow(acc)+1,] = get_perf_metrics(mouse.test, knn.predicted, cm.knn, "kNN")
acc[nrow(acc)+1,] = get_perf_metrics(mouse.test, rf.predicted, cm.rf, "Random Forest")
acc[nrow(acc)+1,] =  get_perf_metrics(mouse.test, as.numeric(svm.pred), cm.svm, "SVM")

kable(acc, align = c("l","r", "c", "c", "r", "r", "r", "r", "r"))
```

# Section 6 Discussion. 

We can see that my knn model performs fairly well on the test set despite the test set not including all predictor variables that the knn was trained on and having many more positive compared to negative feedback. 

Because our dataset is highly imbalanced (there are more positive than negative feedbacks), it would be more accurate to use the balanced accuracy as a metric for evaluating our model's performance. We can also use the F1-score as a measure of our models' accuracy. The F1-score compares a model's precision (correct positive predictions relative to the total number of positive predictions) and recall (correct positive predictions relative to the total number of actual positive predictions). A perfect F1-score of 1.0 means a model has perfect precision and recall while the lowest score of 0 means either precision or recall is 0. From the summary of my models' performance, we can see that knn appears to perform the best in terms of balanced accuracy and the F1 score. Τhe AUC (area under the curve) measures the area under the ROC (receiver operating characteristic curve) which plots the True vs False positive rates. An AUC of 0.5 is the same as a model that has equal true and false positive rates, while an AUC of 1 means a model is perfectly accurate. It is a bit concerning to see that the benchmarks performs better in terms of the AUC compared to my other models. 

Though the benchmark model has a high accuracy and balanced accuracy, its F1 score is quite low which could mean that it would not perform as well on a balanced dataset (equal distribution of classes). The best model appears to be the *k-Nearest Neighbors** as it has the highest accuracy, balanced accuracy, and AUC and has a relatively high F1 score compared to the other models. 

We can see that the non-parametric models (knn, Random Forest, and SVM) perform better than the parametric models (log regression, LDA) which implies that the data is non-linear as that is one of the major assumptions of parametric models. Non-parametric models do not estimate a fixed set of parameters. Instead, they rely on the available training data to derive the model's structure or make predictions. This can be problematic if the dataset is small or noisy as this can lead to overfitting. [3]

Precision refers to a model's ability to accurately predict positive results while recall and sensitivity refers to a model's ability to find all positive results. Specificity refers to a model's ability to find all negative results. These are especially useful metrics for evaluating a model's performance on an unbalanced dataset as a model may perform differently when predicting positive vs negative feedback. We can see that my models tend to perform better in terms of specificity, sensitivity, and recall which means that it is better able to find all possible positive results though it is less able to discern positive from negative feedback. 


# Acknowledgement
	1.  Sai Suresh
	2.  Michelle Wong
	3.  ChatGPT
	4.  https://www.datacamp.com/tutorial/pca-analysis-r
	5.  https://www.statology.org/train-test-split-r/
	6.  https://www.geeksforgeeks.org/linear-discriminant-analysis-in-r-rogramming/
	7.  https://towardsdatascience.com/k-nearest-neighbors-algorithm-with-xamples-in-r-simply-explained-knn-1f2c88da405c
	8.  https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-achine-learning-normalization-standardization/
	9. https://www.r-bloggers.com/2022/07/how-to-standardize-data-in-r/
	10.  https://www.digitalocean.com/community/tutorials/normalize-data-in-
	11.  https://www.statology.org/lasso-regression-in-r/
	12.  https://www.statology.org/logistic-regression-in-r/
	13.  https://homepages.uc.edu/~lis6/Teaching/ML19Spring/Lab/lab7_logit.html#naive-choice-of-cut-off-probability
	14.  https://rpubs.com/harshaash/logistic_regression
	15.  https://rpubs.com/esobolewska/pcr-step-by-step
	16.  https://machinelearningmastery.com/roc-curves-and-precision-recall-urves-for-classification-in-python/
	17.  https://xiaorui.site/Data-Mining-R/lecture/4.D_LogisticReg_ROC.html
	18.  https://www.geeksforgeeks.org/cross-validation-in-r-programming/#
	19.  https://stats.stackexchange.com/questions/318968/knn-and-k-folding-n-r
	20.  https://builtin.com/data-science/bootstrapping-statistics
	21. https://towardsdatascience.com/a-practical-guide-to-bootstrap-with-r-examples-bd975ec6dce
	22. https://www.statology.org/lasso-regression-in-r/
	23. https://www.listendata.com/2014/11/random-forest-with-r.html#id-77c3f2
	24. https://www.geeksforgeeks.org/how-to-remove-outliers-from-multiple-columns-in-r-dataframe/#
	25. https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-r/#
	26. https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf
	27. https://www.r-bloggers.com/2019/04/methods-for-dealing-with-imbalanced-data/
	28. https://plos.figshare.com/articles/figure/Variable_importance_plot_mean_decrease_accuracy_and_mean_decrease_Gini_/12060105/1#:~:text=The%20mean%20decrease%20in%20Gini,the%20variable%20in%20the%20model.
	29. https://neptune.ai/blog/knn-algorithm-explanation-opportunities-limitations#:~:text=This%20is%20much%20different%20from,metrics%20to%20generate%20accurate%20predictions.
  30. https://nancyyanyu.github.io/posts/6084c2b2/
  



# Appendix
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

